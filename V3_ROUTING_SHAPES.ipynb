{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NEO4J DESKTOP --VERSIONE CON APPROSSIMAZIONE\n",
        "\n",
        "ASSUMIAMO CHE SHAPE SIA UNA PROPRIETA DELLA RELAZIONE TRA DUE NODI"
      ],
      "metadata": {
        "id": "ba1M3CwDw4HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 1: IMPORTAZIONI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, LineString\n",
        "import rasterio\n",
        "from neo4j import GraphDatabase\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"Librerie caricate correttamente.\")"
      ],
      "metadata": {
        "id": "DCfq9svHxhmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 2: CONFIGURAZIONE LOCALE\n",
        "# --- CONFIGURAZIONE NEO4J DESKTOP ---\n",
        "URI = \"bolt://localhost:7687\"\n",
        "USERNAME = \"neo4j\"\n",
        "PASSWORD = \"LA_TUA_PASSWORD_LOCALE\" # <--- Inserisci la tua password qui\n",
        "\n",
        "# --- FILE LOCALI ---\n",
        "# IMPORTANTE: I file devono trovarsi nella STESSA CARTELLA di questo notebook\n",
        "# --- FILE LOCALI ---\n",
        "DATA_PATH= '/content/data/'\n",
        "FILES = {\n",
        "    'agency': DATA_PATH +'agency.txt',\n",
        "    'routes': DATA_PATH +'routes.txt',\n",
        "    'trips': DATA_PATH+ 'trips.txt',\n",
        "    'stops': DATA_PATH + 'stops.txt',\n",
        "    'stop_times': DATA_PATH + 'stop_times.txt',\n",
        "    'shapes': DATA_PATH + 'shapes.txt',\n",
        "    'calendar': DATA_PATH + 'new_calendar_dates.txt',\n",
        "    'geojson': DATA_PATH+ 'QuartieriModena.geojson'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RASTER_FILES = {\n",
        "    'morning': DATA_PATH +'crash_risk_morning.tif',\n",
        "    'afternoon': DATA_PATH +'crash_risk_afternoon_raster.tif',\n",
        "    'night': DATA_PATH + 'crash_risk_night_raster.tif'\n",
        "}\n",
        "\n",
        "# --- TEST CONNESSIONE ---\n",
        "driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD))\n",
        "\n",
        "def verify_connection(driver):\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            result = session.run(\"RETURN 'Connessione Locale OK' AS status\")\n",
        "            print(result.single()['status'])\n",
        "    except Exception as e:\n",
        "        print(f\"ERRORE DI CONNESSIONE: {e}\")\n",
        "        print(\"Assicurati che il database in Neo4j Desktop sia avviato (Start).\")\n",
        "\n",
        "verify_connection(driver)"
      ],
      "metadata": {
        "id": "ydxi_xPcxi7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 3: HELPER FUNCTIONS\n",
        "def load_dataframe_to_neo4j(session, query, df, batch_size=2000, **kwargs):\n",
        "    \"\"\"Carica DataFrame in Neo4j a blocchi.\"\"\"\n",
        "    total = len(df)\n",
        "    if total == 0:\n",
        "        print(\"   Warning: DataFrame vuoto, nulla da caricare.\")\n",
        "        return\n",
        "\n",
        "    print(f\"   Caricamento {total} righe in Neo4j...\")\n",
        "    # Converti NaN in None per Cypher e forza conversione tipi base\n",
        "    data = df.where(pd.notnull(df), None).to_dict('records')\n",
        "\n",
        "    start = time.time()\n",
        "    for i in range(0, total, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        session.run(query, batch=batch, **kwargs)\n",
        "        print(f\"   Progresso: {min(i+batch_size, total)}/{total}\", end='\\r')\n",
        "\n",
        "    print(f\"\\n   Fatto in {time.time() - start:.2f}s.\")"
      ],
      "metadata": {
        "id": "F48BLI1ZxlE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 4: CARICAMENTO E FILTRAGGIO DATI (SOLO URBANI)\n",
        "def load_and_filter_data():\n",
        "    print(\"Caricamento CSV e Filtraggio...\")\n",
        "\n",
        "    # 1. ROUTES: Filtra per lista specifica\n",
        "    routes = pd.read_csv(FILES['routes'], dtype=str)\n",
        "\n",
        "    # Lista linee urbane Modena\n",
        "    TARGET_LINES = ['1', '2', '3', '4', '5', '5taxi', '6', '7', '7A', '8', '9', '10', '10taxi', '11', '12', '13']\n",
        "\n",
        "    # Pulizia stringhe\n",
        "    routes['route_short_name'] = routes['route_short_name'].astype(str).str.strip()\n",
        "\n",
        "    # Filtro\n",
        "    urban_routes = routes[routes['route_short_name'].isin(TARGET_LINES)].copy()\n",
        "\n",
        "    print(f\"   Routes Totali: {len(routes)} -> Selezionate: {len(urban_routes)}\")\n",
        "    if len(urban_routes) == 0:\n",
        "        print(\"ERRORE: Nessuna linea trovata! Verifica i nomi nel file routes.txt.\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    valid_route_ids = set(urban_routes['route_id'])\n",
        "\n",
        "    # 2. TRIPS: Solo quelli delle rotte selezionate\n",
        "    trips = pd.read_csv(FILES['trips'], dtype=str)\n",
        "    trips = trips[trips['route_id'].isin(valid_route_ids)]\n",
        "    print(f\"   Trips mantenuti: {len(trips)}\")\n",
        "    valid_trip_ids = set(trips['trip_id'])\n",
        "\n",
        "    # 3. STOP_TIMES: Solo quelli dei trip selezionati\n",
        "    stoptimes = pd.read_csv(FILES['stop_times'], dtype={'trip_id': str, 'stop_id': str, 'stop_sequence': int})\n",
        "    stoptimes = stoptimes[stoptimes['trip_id'].isin(valid_trip_ids)]\n",
        "    stoptimes = stoptimes.sort_values(by=['trip_id', 'stop_sequence']) # Importante ordinare\n",
        "    print(f\"   StopTimes mantenuti: {len(stoptimes)}\")\n",
        "\n",
        "    # 4. STOPS: Solo quelle usate\n",
        "    valid_stop_ids = set(stoptimes['stop_id'])\n",
        "    stops = pd.read_csv(FILES['stops'], dtype={'stop_id': str})\n",
        "    stops = stops[stops['stop_id'].isin(valid_stop_ids)]\n",
        "    print(f\"   Stops mantenute: {len(stops)}\")\n",
        "\n",
        "    # 5. SHAPES: Solo quelle usate dai trip\n",
        "    valid_shape_ids = set(trips['shape_id'].dropna())\n",
        "    shapes = pd.read_csv(FILES['shapes'], dtype={'shape_id': str})\n",
        "    shapes = shapes[shapes['shape_id'].isin(valid_shape_ids)]\n",
        "    shapes = shapes.sort_values(by=['shape_id', 'shape_pt_sequence'])\n",
        "    print(f\"   Punti Shape mantenuti: {len(shapes)}\")\n",
        "\n",
        "    return urban_routes, trips, stoptimes, stops, shapes\n",
        "\n",
        "# Esegui\n",
        "df_routes, df_trips, df_st, df_stops, df_shapes = load_and_filter_data()"
      ],
      "metadata": {
        "id": "GFX_gjiRxnbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 5: ELABORAZIONE SHAPES E RISCHIO (IN MEMORIA)\n",
        "def process_shapes_and_risks(df_shapes, df_st, df_stops, raster_files):\n",
        "    if df_shapes is None or len(df_shapes) == 0:\n",
        "        print(\"Nessuna shape da elaborare.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(\"Apertura Raster e campionamento punti...\")\n",
        "\n",
        "    # 1. Apri Raster\n",
        "    datasets = {}\n",
        "    for k, v in raster_files.items():\n",
        "        try:\n",
        "            datasets[k] = rasterio.open(v)\n",
        "        except:\n",
        "            print(f\"Warning: Impossibile aprire {v}\")\n",
        "\n",
        "    # 2. Arricchisci Shapes con rischio (Loop sui punti)\n",
        "    # Per velocizzare, creiamo colonne numpy\n",
        "    lons = df_shapes['shape_pt_lon'].values\n",
        "    lats = df_shapes['shape_pt_lat'].values\n",
        "\n",
        "    for k, ds in datasets.items():\n",
        "        risk_vals = []\n",
        "        # Lettura raster (può essere lenta, ma su dati filtrati è ok)\n",
        "        for lon, lat in zip(lons, lats):\n",
        "            try:\n",
        "                r, c = ds.index(lon, lat)\n",
        "                val = ds.read(1)[r, c]\n",
        "                risk_vals.append(float(val) if val >= 0 else 0.0)\n",
        "            except:\n",
        "                risk_vals.append(0.0)\n",
        "        df_shapes[f'risk_{k}'] = risk_vals\n",
        "\n",
        "    # Chiudi raster\n",
        "    for ds in datasets.values(): ds.close()\n",
        "\n",
        "    print(\"Creazione segmenti Stop-to-Stop...\")\n",
        "\n",
        "    # Lookup veloci\n",
        "    shapes_dict = {k: v for k, v in df_shapes.groupby('shape_id')}\n",
        "    stops_loc = df_stops.set_index('stop_id')[['stop_lat', 'stop_lon']].to_dict('index')\n",
        "\n",
        "    precedes_data = []\n",
        "    grouped_st = df_st.groupby('trip_id')\n",
        "\n",
        "    # Trip Lookup\n",
        "    trip_shape_map = df_trips.set_index('trip_id')['shape_id'].to_dict()\n",
        "\n",
        "    count = 0\n",
        "    total = len(grouped_st)\n",
        "\n",
        "    for trip_id, group in grouped_st:\n",
        "        count += 1\n",
        "        if count % 200 == 0: print(f\"   Elaborati {count}/{total} trips...\", end='\\r')\n",
        "\n",
        "        shape_id = trip_shape_map.get(trip_id)\n",
        "        stops_seq = group.to_dict('records')\n",
        "\n",
        "        # Itera coppie di stop (A -> B)\n",
        "        for i in range(len(stops_seq) - 1):\n",
        "            st_A = stops_seq[i]\n",
        "            st_B = stops_seq[i+1]\n",
        "\n",
        "            props = {\n",
        "                'trip_id': trip_id,\n",
        "                'stop_seq_A': st_A['stop_sequence'],\n",
        "                'departure_time': st_A['departure_time'],\n",
        "                'arrival_time': st_B['arrival_time'],\n",
        "                'geometry': None,\n",
        "                'avg_risk_morning': 0.0, 'avg_risk_afternoon': 0.0,\n",
        "                'avg_risk_night': 0.0\n",
        "            }\n",
        "\n",
        "            # Se abbiamo la shape, tagliamo il segmento\n",
        "            if shape_id and shape_id in shapes_dict:\n",
        "                pts = shapes_dict[shape_id]\n",
        "                latA, lonA = stops_loc[st_A['stop_id']]['stop_lat'], stops_loc[st_A['stop_id']]['stop_lon']\n",
        "                latB, lonB = stops_loc[st_B['stop_id']]['stop_lat'], stops_loc[st_B['stop_id']]['stop_lon']\n",
        "\n",
        "                # Trova indici più vicini (Euclidea quadrata)\n",
        "                distsA = (pts['shape_pt_lat'] - latA)**2 + (pts['shape_pt_lon'] - lonA)**2\n",
        "                distsB = (pts['shape_pt_lat'] - latB)**2 + (pts['shape_pt_lon'] - lonB)**2\n",
        "\n",
        "                idx_A = distsA.idxmin()\n",
        "                idx_B = distsB.idxmin()\n",
        "\n",
        "                # Gestione ordine (se idx_A > idx_B c'è qualcosa di strano, ma prendiamo il range comunque)\n",
        "                start, end = min(idx_A, idx_B), max(idx_A, idx_B)\n",
        "\n",
        "                # Estrai segmento\n",
        "                segment = pts.loc[start:end]\n",
        "\n",
        "                if not segment.empty:\n",
        "                    # Calcola medie rischi\n",
        "                    for t in ['morning', 'afternoon', 'night']:\n",
        "                        props[f'avg_risk_{t}'] = segment[f'risk_{t}'].mean()\n",
        "\n",
        "                    # Geometria WKT\n",
        "                    coords = list(zip(segment['shape_pt_lon'], segment['shape_pt_lat']))\n",
        "                    if len(coords) < 2: coords.append(coords[0]) # Evita punti singoli\n",
        "                    props['geometry'] = str(LineString(coords))\n",
        "\n",
        "            precedes_data.append(props)\n",
        "\n",
        "    print(\"\\nElaborazione completata.\")\n",
        "    return pd.DataFrame(precedes_data)\n",
        "\n",
        "# Esegui\n",
        "df_precedes_enriched = process_shapes_and_risks(df_shapes, df_st, df_stops, RASTER_FILES)"
      ],
      "metadata": {
        "id": "i41MLdqSxr8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 6: SETUP DB\n",
        "def setup_db(driver):\n",
        "    with driver.session() as session:\n",
        "        # PULIZIA TOTALE (Decommenta se vuoi resettare il DB ogni volta)\n",
        "        # print(\"Pulizia Database...\")\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "        print(\"Creazione Vincoli...\")\n",
        "        cmds = [\n",
        "            \"CREATE CONSTRAINT agency_id IF NOT EXISTS FOR (a:Agency) REQUIRE a.id IS UNIQUE\",\n",
        "            \"CREATE CONSTRAINT route_id IF NOT EXISTS FOR (r:Route) REQUIRE r.id IS UNIQUE\",\n",
        "            \"CREATE CONSTRAINT trip_id IF NOT EXISTS FOR (t:Trip) REQUIRE t.id IS UNIQUE\",\n",
        "            \"CREATE CONSTRAINT stop_id IF NOT EXISTS FOR (s:Stop) REQUIRE s.id IS UNIQUE\",\n",
        "            \"CREATE CONSTRAINT stoptime_id IF NOT EXISTS FOR (st:Stoptime) REQUIRE (st.trip_id, st.stop_sequence) IS UNIQUE\"\n",
        "        ]\n",
        "        for c in cmds: session.run(c)\n",
        "        print(\"DB Pronto.\")\n",
        "\n",
        "setup_db(driver)"
      ],
      "metadata": {
        "id": "7efJpmkAxv0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 7: CARICAMENTO DATI IN NEO4J\n",
        "def upload_graph(driver, df_routes, df_trips, df_stops, df_st, df_precedes_enriched):\n",
        "    with driver.session() as session:\n",
        "        # 1. Agency (Default ID '1' se manca)\n",
        "        df_agency = pd.read_csv(FILES['agency'], dtype=str)\n",
        "        if 'agency_id' not in df_agency.columns: df_agency['agency_id'] = '1'\n",
        "        q_ag = \"\"\"UNWIND $batch AS row MERGE (a:Agency {id: row.agency_id}) SET a.name = row.agency_name\"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_ag, df_agency)\n",
        "\n",
        "        # 2. Routes\n",
        "        q_routes = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH (a:Agency) LIMIT 1\n",
        "        MERGE (r:Route {id: row.route_id})\n",
        "        SET r.short_name = row.route_short_name, r.long_name = row.route_long_name\n",
        "        MERGE (a)-[:OPERATES]->(r)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_routes, df_routes)\n",
        "\n",
        "        # 3. Trips\n",
        "        q_trips = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH (r:Route {id: row.route_id})\n",
        "        MERGE (t:Trip {id: row.trip_id})\n",
        "        SET t.headsign = row.trip_headsign, t.service_id = row.service_id\n",
        "        MERGE (r)<-[:USES]-(t)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_trips, df_trips)\n",
        "\n",
        "        # 4. Stops\n",
        "        q_stops = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MERGE (s:Stop {id: row.stop_id})\n",
        "        SET s.name = row.stop_name, s.lat = toFloat(row.stop_lat), s.lon = toFloat(row.stop_lon)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_stops, df_stops)\n",
        "\n",
        "        # 5. StopTimes (Nodi)\n",
        "        print(\"Caricamento nodi Stoptime...\")\n",
        "        q_st = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH (t:Trip {id: row.trip_id}), (s:Stop {id: row.stop_id})\n",
        "        MERGE (st:Stoptime {trip_id: row.trip_id, stop_sequence: toInteger(row.stop_sequence)})\n",
        "        SET st.arrival_time = time(row.arrival_time), st.departure_time = time(row.departure_time)\n",
        "        MERGE (t)<-[:PART_OF_TRIP]-(st)\n",
        "        MERGE (st)-[:LOCATED_AT]->(s)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_st, df_st)\n",
        "\n",
        "        # 6. RELAZIONE PRECEDES (con Geometria e Rischio)\n",
        "        print(\"Caricamento relazioni PRECEDES arricchite...\")\n",
        "        q_precedes = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH (stA:Stoptime {trip_id: row.trip_id, stop_sequence: toInteger(row.stop_seq_A)})\n",
        "        MATCH (stB:Stoptime {trip_id: row.trip_id, stop_sequence: toInteger(row.stop_seq_A) + 1})\n",
        "        MERGE (stA)-[p:PRECEDES]->(stB)\n",
        "        SET p.geometry = row.geometry,\n",
        "            p.waiting_time = duration.inSeconds(time(row.departure_time), time(row.arrival_time)).seconds,\n",
        "            p.risk_morning = toFloat(row.avg_risk_morning),\n",
        "            p.risk_afternoon = toFloat(row.avg_risk_afternoon),\n",
        "            p.risk_evening = toFloat(row.avg_risk_evening),\n",
        "            p.risk_night = toFloat(row.avg_risk_night)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_precedes, df_precedes_enriched)\n",
        "\n",
        "        # 7. WALK_TO\n",
        "        print(\"Creazione WALK_TO (< 300m)...\")\n",
        "        session.run(\"\"\"\n",
        "            MATCH (s1:Stop), (s2:Stop)\n",
        "            WHERE elementId(s1) < elementId(s2)\n",
        "            WITH s1, s2, point.distance(point({latitude: s1.lat, longitude: s1.lon}),\n",
        "                                        point({latitude: s2.lat, longitude: s2.lon})) AS dist\n",
        "            WHERE dist < 300\n",
        "            MERGE (s1)-[:WALK_TO {distance: dist}]->(s2)\n",
        "            MERGE (s2)-[:WALK_TO {distance: dist}]->(s1)\n",
        "        \"\"\")\n",
        "\n",
        "upload_graph(driver, df_routes, df_trips, df_stops, df_st, df_precedes_enriched)"
      ],
      "metadata": {
        "id": "qZLK84i2xxW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 8: CARICAMENTO QUARTIERI\n",
        "def load_neighborhoods(driver):\n",
        "    print(\"Caricamento GeoJSON Quartieri...\")\n",
        "    gdf = gpd.read_file(FILES['geojson']).to_crs(\"EPSG:4326\")\n",
        "\n",
        "    neighborhoods = []\n",
        "    for idx, row in gdf.iterrows():\n",
        "        centroid = row.geometry.centroid\n",
        "        name = row.get('nome', f\"Q_{idx}\")\n",
        "        neighborhoods.append({'name': name, 'lat': centroid.y, 'lon': centroid.x})\n",
        "\n",
        "    with driver.session() as session:\n",
        "        q = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MERGE (n:Neighborhood {name: row.name})\n",
        "        SET n.lat = row.lat, n.lon = row.lon\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q, pd.DataFrame(neighborhoods))\n",
        "\n",
        "        # Collegamento Access Point\n",
        "        session.run(\"\"\"\n",
        "            MATCH (n:Neighborhood)\n",
        "            WITH n, point({latitude: n.lat, longitude: n.lon}) as pN\n",
        "            MATCH (s:Stop)\n",
        "            WITH n, s, point.distance(pN, point({latitude: s.lat, longitude: s.lon})) as dist\n",
        "            ORDER BY dist ASC\n",
        "            WITH n, collect(s)[0] as closest, dist\n",
        "            MERGE (n)-[:ACCESS_POINT {distance: dist}]->(closest)\n",
        "        \"\"\")\n",
        "    print(\"Quartieri collegati.\")\n",
        "\n",
        "load_neighborhoods(driver)"
      ],
      "metadata": {
        "id": "ftMg2HT5x0dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 9: ROUTING DIJKSTRA (TIME + RISK)\n",
        "def run_routing(driver, time_of_day='morning', risk_penalty=1000):\n",
        "    print(f\"--- Calcolo Percorsi: {time_of_day.upper()} (Penalità Rischio: {risk_penalty}) ---\")\n",
        "\n",
        "    # Nome proprietà rischio\n",
        "    risk_prop = f\"risk_{time_of_day}\"\n",
        "\n",
        "    # Formula Peso: secondi + (probabilità * penalità)\n",
        "    # Usa coalesce per gestire eventuali nulli come rischio 0\n",
        "    weight_q = f\"waiting_time + (coalesce({risk_prop}, 0.0) * {risk_penalty})\"\n",
        "\n",
        "    graph_name = f\"urban_simpler_{time_of_day}\"\n",
        "\n",
        "    with driver.session() as session:\n",
        "        # 1. Drop proiezione precedente\n",
        "        session.run(f\"CALL gds.graph.drop('{graph_name}', false)\")\n",
        "\n",
        "        # 2. Proiezione Grafo\n",
        "        print(\"Proiezione Grafo in memoria...\")\n",
        "        session.run(f\"\"\"\n",
        "            CALL gds.graph.project(\n",
        "                '{graph_name}',\n",
        "                ['Neighborhood', 'Stop', 'Stoptime'],\n",
        "                {{\n",
        "                    PRECEDES: {{\n",
        "                        type: 'PRECEDES',\n",
        "                        properties: {{ weight: '{weight_q}' }}\n",
        "                    }},\n",
        "                    WALK_TO: {{\n",
        "                        type: 'WALK_TO',\n",
        "                        properties: {{ weight: 'distance / 1.3' }}\n",
        "                    }},\n",
        "                    ACCESS_POINT: {{\n",
        "                        type: 'ACCESS_POINT',\n",
        "                        orientation: 'UNDIRECTED',\n",
        "                        properties: {{ weight: 'distance / 1.3' }}\n",
        "                    }},\n",
        "                    LOCATED_AT: {{ type: 'LOCATED_AT', orientation: 'UNDIRECTED' }},\n",
        "                    PART_OF_TRIP: {{ type: 'PART_OF_TRIP', orientation: 'UNDIRECTED' }}\n",
        "                }}\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        # 3. Esecuzione Dijkstra\n",
        "        print(\"Calcolo Shortest Path...\")\n",
        "        result = session.run(f\"\"\"\n",
        "            MATCH (n1:Neighborhood), (n2:Neighborhood)\n",
        "            WHERE elementId(n1) < elementId(n2)\n",
        "\n",
        "            CALL gds.shortestPath.dijkstra.stream('{graph_name}', {{\n",
        "                sourceNode: n1,\n",
        "                targetNode: n2,\n",
        "                relationshipWeightProperty: 'weight'\n",
        "            }})\n",
        "            YIELD totalCost\n",
        "\n",
        "            RETURN n1.name as Da, n2.name as A, totalCost as Score_Totale\n",
        "            LIMIT 20\n",
        "        \"\"\")\n",
        "\n",
        "        df_res = pd.DataFrame([r.data() for r in result])\n",
        "\n",
        "        # Cleanup\n",
        "        session.run(f\"CALL gds.graph.drop('{graph_name}', false)\")\n",
        "        return df_res\n",
        "\n",
        "# Esegui analisi\n",
        "df_results = run_routing(driver, 'morning', risk_penalty=1000)\n",
        "print(df_results)"
      ],
      "metadata": {
        "id": "3cACbn96xzw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEO4J DESKTOP-- VERSIONE CHE CREA SHAPE POINT"
      ],
      "metadata": {
        "id": "L6EL-fwpylSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 1: IMPORTAZIONI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import rasterio\n",
        "from neo4j import GraphDatabase\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"Librerie caricate correttamente.\")"
      ],
      "metadata": {
        "id": "_NXZsJCYyv74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 2: CONFIGURAZIONE LOCALE\n",
        "# --- CONFIGURAZIONE NEO4J DESKTOP ---\n",
        "URI = \"bolt://localhost:7687\"\n",
        "USERNAME = \"neo4j\"\n",
        "PASSWORD = \"LA_TUA_PASSWORD_LOCALE\" # <--- Inserisci la tua password qui\n",
        "\n",
        "# --- FILE LOCALI ---\n",
        "# IMPORTANTE: I file devono trovarsi nella STESSA CARTELLA di questo notebook\n",
        "# --- FILE LOCALI ---\n",
        "DATA_PATH= '/content/data/'\n",
        "FILES = {\n",
        "    'agency': DATA_PATH +'agency.txt',\n",
        "    'routes': DATA_PATH +'routes.txt',\n",
        "    'trips': DATA_PATH+ 'trips.txt',\n",
        "    'stops': DATA_PATH + 'stops.txt',\n",
        "    'stop_times': DATA_PATH + 'stop_times.txt',\n",
        "    'shapes': DATA_PATH + 'shapes.txt',\n",
        "    'calendar': DATA_PATH + 'new_calendar_dates.txt',\n",
        "    'geojson': DATA_PATH+ 'QuartieriModena.geojson'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RASTER_FILES = {\n",
        "    'morning': DATA_PATH +'crash_risk_morning.tif',\n",
        "    'afternoon': DATA_PATH +'crash_risk_afternoon_raster.tif',\n",
        "    'night': DATA_PATH + 'crash_risk_night_raster.tif'\n",
        "}\n",
        "\n",
        "# --- TEST CONNESSIONE ---\n",
        "driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD))\n",
        "\n",
        "def verify_connection(driver):\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            result = session.run(\"RETURN 'Connessione Locale OK' AS status\")\n",
        "            print(result.single()['status'])\n",
        "    except Exception as e:\n",
        "        print(f\"ERRORE DI CONNESSIONE: {e}\")\n",
        "        print(\"Assicurati che il database in Neo4j Desktop sia avviato (Start).\")\n",
        "\n",
        "verify_connection(driver)"
      ],
      "metadata": {
        "id": "cI8zojFfywxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 3: HELPER LOAD BATCH\n",
        "def load_dataframe_to_neo4j(session, query, df, batch_size=2000, **kwargs):\n",
        "    \"\"\"Carica DataFrame in Neo4j a blocchi.\"\"\"\n",
        "    total = len(df)\n",
        "    if total == 0: return\n",
        "\n",
        "    print(f\"   Caricamento {total} righe...\")\n",
        "    # Gestione NaN -> None per Cypher\n",
        "    data = df.where(pd.notnull(df), None).to_dict('records')\n",
        "\n",
        "    start = time.time()\n",
        "    for i in range(0, total, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        session.run(query, batch=batch, **kwargs)\n",
        "        print(f\"   Progresso: {min(i+batch_size, total)}/{total}\", end='\\r')\n",
        "\n",
        "    print(f\"\\n   Completato in {time.time() - start:.2f}s.\")"
      ],
      "metadata": {
        "id": "50LzKjlXy2Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 4: FILTRAGGIO DATI (SOLO LINEE URBANE)\n",
        "def load_and_filter_data():\n",
        "    print(\"Caricamento e Filtraggio Dati...\")\n",
        "\n",
        "    # 1. ROUTES\n",
        "    routes = pd.read_csv(FILES['routes'], dtype=str)\n",
        "\n",
        "    # TUE LINEE URBANE\n",
        "    TARGET_LINES = ['1', '2', '3', '4', '5', '5taxi', '6', '7', '7A', '8', '9', '10', '10taxi', '11', '12', '13']\n",
        "\n",
        "    # Pulizia e Filtro\n",
        "    routes['route_short_name'] = routes['route_short_name'].astype(str).str.strip()\n",
        "    urban_routes = routes[routes['route_short_name'].isin(TARGET_LINES)].copy()\n",
        "\n",
        "    print(f\"   Routes: {len(routes)} -> {len(urban_routes)} (Urbane)\")\n",
        "\n",
        "    if len(urban_routes) == 0:\n",
        "        print(\"ERRORE: Nessuna linea trovata. Controlla i nomi in routes.txt\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    valid_route_ids = set(urban_routes['route_id'])\n",
        "\n",
        "    # 2. TRIPS\n",
        "    trips = pd.read_csv(FILES['trips'], dtype=str)\n",
        "    trips = trips[trips['route_id'].isin(valid_route_ids)]\n",
        "    print(f\"   Trips: {len(trips)}\")\n",
        "    valid_trip_ids = set(trips['trip_id'])\n",
        "\n",
        "    # 3. STOP_TIMES\n",
        "    stoptimes = pd.read_csv(FILES['stop_times'], dtype={'trip_id': str, 'stop_id': str, 'stop_sequence': int})\n",
        "    stoptimes = stoptimes[stoptimes['trip_id'].isin(valid_trip_ids)]\n",
        "    print(f\"   StopTimes: {len(stoptimes)}\")\n",
        "\n",
        "    # 4. STOPS\n",
        "    valid_stop_ids = set(stoptimes['stop_id'])\n",
        "    stops = pd.read_csv(FILES['stops'], dtype={'stop_id': str})\n",
        "    stops = stops[stops['stop_id'].isin(valid_stop_ids)]\n",
        "    print(f\"   Stops: {len(stops)}\")\n",
        "\n",
        "    # 5. SHAPES (PUNTI GEOMETRICI)\n",
        "    # Teniamo solo i punti delle shape usate dai trip urbani\n",
        "    valid_shape_ids = set(trips['shape_id'].dropna())\n",
        "    shapes = pd.read_csv(FILES['shapes'], dtype={'shape_id': str})\n",
        "    shapes = shapes[shapes['shape_id'].isin(valid_shape_ids)]\n",
        "    # Ordine essenziale per la sequenza\n",
        "    shapes = shapes.sort_values(by=['shape_id', 'shape_pt_sequence'])\n",
        "    print(f\"   ShapePoints: {len(shapes)}\")\n",
        "\n",
        "    return urban_routes, trips, stoptimes, stops, shapes\n",
        "\n",
        "# Esegui\n",
        "df_routes, df_trips, df_st, df_stops, df_shapes = load_and_filter_data()"
      ],
      "metadata": {
        "id": "xJDpBN1sy5mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 5: CARICAMENTO GRAFO COMPLETO (NODES + SHAPEPOINTS)\n",
        "def upload_full_graph(driver, df_routes, df_trips, df_stops, df_shapes):\n",
        "    with driver.session() as session:\n",
        "        # Pulizia opzionale (Decommenta per resettare)\n",
        "        # session.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "        # Vincoli\n",
        "        print(\"Creazione Vincoli...\")\n",
        "        cmds = [\n",
        "            \"CREATE CONSTRAINT agency_id IF NOT EXISTS FOR (a:Agency) REQUIRE a.id IS UNIQUE\",\n",
        "            \"CREATE CONSTRAINT route_id IF NOT EXISTS FOR (r:Route) REQUIRE r.id IS UNIQUE\",\n",
        "            \"CREATE CONSTRAINT trip_id IF NOT EXISTS FOR (t:Trip) REQUIRE t.id IS UNIQUE\",\n",
        "            \"CREATE CONSTRAINT stop_id IF NOT EXISTS FOR (s:Stop) REQUIRE s.id IS UNIQUE\",\n",
        "            # Vincolo composto per ShapePoints (ShapeID + Sequenza)\n",
        "            \"CREATE CONSTRAINT shape_pt_id IF NOT EXISTS FOR (sp:ShapePoint) REQUIRE (sp.shape_id, sp.sequence) IS UNIQUE\"\n",
        "        ]\n",
        "        for c in cmds: session.run(c)\n",
        "\n",
        "        # 1. Agency\n",
        "        print(\"1. Agency...\")\n",
        "        df_ag = pd.read_csv(FILES['agency'], dtype=str)\n",
        "        if 'agency_id' not in df_ag.columns: df_ag['agency_id'] = '1'\n",
        "        q_ag = \"UNWIND $batch as row MERGE (a:Agency {id: row.agency_id}) SET a.name = row.agency_name\"\n",
        "        load_dataframe_to_neo4j(session, q_ag, df_ag)\n",
        "\n",
        "        # 2. Routes\n",
        "        print(\"2. Routes...\")\n",
        "        q_r = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH (a:Agency) LIMIT 1\n",
        "        MERGE (r:Route {id: row.route_id})\n",
        "        SET r.short_name = row.route_short_name, r.long_name = row.route_long_name\n",
        "        MERGE (a)-[:OPERATES]->(r)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_r, df_routes)\n",
        "\n",
        "        # 3. Trips\n",
        "        print(\"3. Trips...\")\n",
        "        q_t = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH (r:Route {id: row.route_id})\n",
        "        MERGE (t:Trip {id: row.trip_id})\n",
        "        SET t.headsign = row.trip_headsign, t.service_id = row.service_id, t.shape_id = row.shape_id\n",
        "        MERGE (r)<-[:USES]-(t)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_t, df_trips)\n",
        "\n",
        "        # 4. Stops\n",
        "        print(\"4. Stops...\")\n",
        "        q_s = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MERGE (s:Stop {id: row.stop_id})\n",
        "        SET s.lat = toFloat(row.stop_lat), s.lon = toFloat(row.stop_lon), s.name = row.stop_name\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_s, df_stops)\n",
        "\n",
        "        # 5. SHAPE POINTS (Il cuore di questa versione)\n",
        "        print(\"5. ShapePoints (Nodi geometrici)...\")\n",
        "        q_sp = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MERGE (sp:ShapePoint {shape_id: row.shape_id, sequence: toInteger(row.shape_pt_sequence)})\n",
        "        SET sp.lat = toFloat(row.shape_pt_lat), sp.lon = toFloat(row.shape_pt_lon)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_sp, df_shapes, batch_size=5000)\n",
        "\n",
        "        print(\"   Collegamento ShapePoints (FOLLOWS)...\")\n",
        "        # Usiamo una query Cypher ottimizzata per collegare la sequenza\n",
        "        # Poiché abbiamo filtrato i dati, questa operazione su Desktop è veloce\n",
        "        session.run(\"\"\"\n",
        "            CALL apoc.periodic.iterate(\n",
        "                \"MATCH (sp:ShapePoint) RETURN sp ORDER BY sp.shape_id, sp.sequence\",\n",
        "                \"WITH sp\n",
        "                 MATCH (prev:ShapePoint {shape_id: sp.shape_id, sequence: sp.sequence - 1})\n",
        "                 MERGE (prev)-[r:FOLLOWS]->(sp)\n",
        "                 SET r.distance = point.distance(point({latitude: prev.lat, longitude: prev.lon}),\n",
        "                                                 point({latitude: sp.lat, longitude: sp.lon}))\",\n",
        "                {batchSize: 2000, parallel: false}\n",
        "            )\n",
        "        \"\"\")\n",
        "        print(\"Grafo geometrico costruito.\")\n",
        "\n",
        "upload_full_graph(driver, df_routes, df_trips, df_stops, df_shapes)"
      ],
      "metadata": {
        "id": "zB0FiIV8y--p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 6: STOPTIMES E WALK\n",
        "def load_schedule(driver, df_st):\n",
        "    with driver.session() as session:\n",
        "        print(\"Caricamento StopTimes...\")\n",
        "        q_st = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH (t:Trip {id: row.trip_id}), (s:Stop {id: row.stop_id})\n",
        "        MERGE (st:Stoptime {trip_id: row.trip_id, stop_sequence: toInteger(row.stop_sequence)})\n",
        "        SET st.arrival_time = time(row.arrival_time), st.departure_time = time(row.departure_time)\n",
        "        MERGE (t)<-[:PART_OF_TRIP]-(st)\n",
        "        MERGE (st)-[:LOCATED_AT]->(s)\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_st, df_st)\n",
        "\n",
        "        print(\"Creazione Relazioni PRECEDES (Base)...\")\n",
        "        session.run(\"\"\"\n",
        "            CALL apoc.periodic.iterate(\n",
        "                \"MATCH (t:Trip) RETURN t\",\n",
        "                \"MATCH (t)<-[:PART_OF_TRIP]-(st1:Stoptime)\n",
        "                 MATCH (t)<-[:PART_OF_TRIP]-(st2:Stoptime)\n",
        "                 WHERE st2.stop_sequence = st1.stop_sequence + 1\n",
        "                 MERGE (st1)-[p:PRECEDES]->(st2)\n",
        "                 SET p.waiting_time = duration.inSeconds(st1.departure_time, st2.arrival_time).seconds\",\n",
        "                {batchSize: 500}\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        print(\"Creazione WALK_TO (< 300m)...\")\n",
        "        session.run(\"\"\"\n",
        "            MATCH (s1:Stop), (s2:Stop)\n",
        "            WHERE elementId(s1) < elementId(s2)\n",
        "            WITH s1, s2, point.distance(point({latitude: s1.lat, longitude: s1.lon}),\n",
        "                                        point({latitude: s2.lat, longitude: s2.lon})) AS dist\n",
        "            WHERE dist < 300\n",
        "            MERGE (s1)-[:WALK_TO {distance: dist}]->(s2)\n",
        "            MERGE (s2)-[:WALK_TO {distance: dist}]->(s1)\n",
        "        \"\"\")\n",
        "\n",
        "load_schedule(driver, df_st)"
      ],
      "metadata": {
        "id": "6fsdkjJ4zBPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 7: INTEGRAZIONE RASTER (SU SHAPE POINTS)\n",
        "def integrate_risk_on_shapes(driver, raster_files):\n",
        "    print(\"Inizio calcolo rischi sui segmenti stradali...\")\n",
        "\n",
        "    # 1. Apri Raster\n",
        "    datasets = {}\n",
        "    for k, v in raster_files.items():\n",
        "        try:\n",
        "            datasets[k] = rasterio.open(v)\n",
        "        except:\n",
        "            print(f\"Warning: {v} non trovato.\")\n",
        "\n",
        "    # 2. Recupera segmenti fisici\n",
        "    print(\"Recupero segmenti FOLLOWS...\")\n",
        "    with driver.session() as session:\n",
        "        res = session.run(\"\"\"\n",
        "            MATCH (s1:ShapePoint)-[r:FOLLOWS]->(s2:ShapePoint)\n",
        "            RETURN elementId(r) as id, s1.lat as lat1, s1.lon as lon1, s2.lat as lat2, s2.lon as lon2\n",
        "        \"\"\")\n",
        "        segments = [r.data() for r in res]\n",
        "\n",
        "    print(f\"Elaborazione di {len(segments)} segmenti...\")\n",
        "    updates = []\n",
        "\n",
        "    for seg in segments:\n",
        "        mid_lat = (seg['lat1'] + seg['lat2']) / 2\n",
        "        mid_lon = (seg['lon1'] + seg['lon2']) / 2\n",
        "\n",
        "        props = {'id': seg['id']}\n",
        "        for k, ds in datasets.items():\n",
        "            try:\n",
        "                r, c = ds.index(mid_lon, mid_lat)\n",
        "                val = ds.read(1)[r, c]\n",
        "                props[f'risk_{k}'] = float(val) if val >= 0 else 0.0\n",
        "            except:\n",
        "                props[f'risk_{k}'] = 0.0\n",
        "        updates.append(props)\n",
        "\n",
        "    # 3. Aggiorna Neo4j\n",
        "    print(\"Salvataggio Rischi nel DB...\")\n",
        "    with driver.session() as session:\n",
        "        q_upd = \"\"\"\n",
        "        UNWIND $batch as row\n",
        "        MATCH ()-[r:FOLLOWS]->() WHERE elementId(r) = row.id\n",
        "        SET r.risk_morning = row.risk_morning,\n",
        "            r.risk_afternoon = row.risk_afternoon,\n",
        "            r.risk_evening = row.risk_evening,\n",
        "            r.risk_night = row.risk_night\n",
        "        \"\"\"\n",
        "        load_dataframe_to_neo4j(session, q_upd, pd.DataFrame(updates), batch_size=5000)\n",
        "\n",
        "    for ds in datasets.values(): ds.close()\n",
        "    print(\"Integrazione Raster completata.\")\n",
        "\n",
        "integrate_risk_on_shapes(driver, RASTER_FILES)"
      ],
      "metadata": {
        "id": "RjHGDX25zFOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 8: PROPAGAZIONE RISCHIO\n",
        "def propagate_risk(driver):\n",
        "    print(\"Propagazione Rischio (Shape -> Trip)...\")\n",
        "\n",
        "    # Calcola media rischio per ShapeID e aggiorna i Trip\n",
        "    query = \"\"\"\n",
        "    CALL apoc.periodic.iterate(\n",
        "        \"MATCH (sp1:ShapePoint)-[r:FOLLOWS]->(sp2:ShapePoint)\n",
        "         RETURN sp1.shape_id AS shape_id,\n",
        "                avg(r.risk_morning) as rm, avg(r.risk_afternoon) as ra,\n",
        "                avg(r.risk_evening) as re, avg(r.risk_night) as rn\",\n",
        "        \"MATCH (t:Trip {shape_id: shape_id})\n",
        "         MATCH (t)<-[:PART_OF_TRIP]-(st1:Stoptime)-[p:PRECEDES]->(st2:Stoptime)\n",
        "         SET p.risk_morning = rm,\n",
        "             p.risk_afternoon = ra,\n",
        "             p.risk_evening = re,\n",
        "             p.risk_night = rn\",\n",
        "        {batchSize: 100, parallel: false}\n",
        "    )\n",
        "    \"\"\"\n",
        "    with driver.session() as session:\n",
        "        session.run(query)\n",
        "    print(\"Propagazione completata.\")\n",
        "\n",
        "propagate_risk(driver)"
      ],
      "metadata": {
        "id": "xnPOQuQwzJXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 9: CARICAMENTO QUARTIERI\n",
        "def load_neighborhoods(driver):\n",
        "    print(\"Caricamento Quartieri...\")\n",
        "    gdf = gpd.read_file(FILES['geojson']).to_crs(\"EPSG:4326\")\n",
        "\n",
        "    neighborhoods = []\n",
        "    for idx, row in gdf.iterrows():\n",
        "        centroid = row.geometry.centroid\n",
        "        neighborhoods.append({'name': row.get('nome', f\"Q_{idx}\"), 'lat': centroid.y, 'lon': centroid.x})\n",
        "\n",
        "    with driver.session() as session:\n",
        "        q = \"UNWIND $batch as row MERGE (n:Neighborhood {name: row.name}) SET n.lat = row.lat, n.lon = row.lon\"\n",
        "        load_dataframe_to_neo4j(session, q, pd.DataFrame(neighborhoods))\n",
        "\n",
        "        session.run(\"\"\"\n",
        "            MATCH (n:Neighborhood)\n",
        "            WITH n, point({latitude: n.lat, longitude: n.lon}) as pN\n",
        "            MATCH (s:Stop)\n",
        "            WITH n, s, point.distance(pN, point({latitude: s.lat, longitude: s.lon})) as dist\n",
        "            ORDER BY dist ASC\n",
        "            WITH n, collect(s)[0] as closest, dist\n",
        "            MERGE (n)-[:ACCESS_POINT {distance: dist}]->(closest)\n",
        "        \"\"\")\n",
        "    print(\"Quartieri pronti.\")\n",
        "\n",
        "load_neighborhoods(driver)"
      ],
      "metadata": {
        "id": "iRhBH-s-zLGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELLA 10: ROUTING DIJKSTRA\n",
        "def run_routing(driver, time_of_day='morning', risk_penalty=1000):\n",
        "    print(f\"--- Routing: {time_of_day.upper()} (Penalità: {risk_penalty}) ---\")\n",
        "\n",
        "    risk_prop = f\"risk_{time_of_day}\"\n",
        "    # Peso = Secondi + (Rischio * Penalità)\n",
        "    weight_q = f\"waiting_time + (coalesce({risk_prop}, 0.0) * {risk_penalty})\"\n",
        "    graph_name = f\"full_geo_graph_{time_of_day}\"\n",
        "\n",
        "    with driver.session() as session:\n",
        "        session.run(f\"CALL gds.graph.drop('{graph_name}', false)\")\n",
        "\n",
        "        session.run(f\"\"\"\n",
        "            CALL gds.graph.project(\n",
        "                '{graph_name}',\n",
        "                ['Neighborhood', 'Stop', 'Stoptime'],\n",
        "                {{\n",
        "                    PRECEDES: {{\n",
        "                        type: 'PRECEDES',\n",
        "                        properties: {{ weight: '{weight_q}' }}\n",
        "                    }},\n",
        "                    WALK_TO: {{\n",
        "                        type: 'WALK_TO',\n",
        "                        properties: {{ weight: 'distance / 1.3' }}\n",
        "                    }},\n",
        "                    ACCESS_POINT: {{\n",
        "                        type: 'ACCESS_POINT',\n",
        "                        orientation: 'UNDIRECTED',\n",
        "                        properties: {{ weight: 'distance / 1.3' }}\n",
        "                    }},\n",
        "                    LOCATED_AT: {{ type: 'LOCATED_AT', orientation: 'UNDIRECTED' }},\n",
        "                    PART_OF_TRIP: {{ type: 'PART_OF_TRIP', orientation: 'UNDIRECTED' }}\n",
        "                }}\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        result = session.run(f\"\"\"\n",
        "            MATCH (n1:Neighborhood), (n2:Neighborhood)\n",
        "            WHERE elementId(n1) < elementId(n2)\n",
        "            CALL gds.shortestPath.dijkstra.stream('{graph_name}', {{\n",
        "                sourceNode: n1, targetNode: n2, relationshipWeightProperty: 'weight'\n",
        "            }})\n",
        "            YIELD totalCost\n",
        "            RETURN n1.name as Da, n2.name as A, totalCost as Score\n",
        "            LIMIT 20\n",
        "        \"\"\")\n",
        "\n",
        "        df_res = pd.DataFrame([r.data() for r in result])\n",
        "        session.run(f\"CALL gds.graph.drop('{graph_name}', false)\")\n",
        "        return df_res\n",
        "\n",
        "# Esegui\n",
        "print(run_routing(driver, 'morning'))"
      ],
      "metadata": {
        "id": "liyD1FobzMcq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}